{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group members\n",
    "\n",
    "- Austin Blanco\n",
    "- Yifei Du\n",
    "- Alvin Xiao\n",
    "\n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project aims to develop and compare reinforcement learning (RL) methods—specifically Q-learning and Deep RL—to train an intelligent agent for Pac-Man. We use the Pac-Man game environment, where the “data” comprises observable states (positions of Pac-Man, ghosts, and pellets) and discrete actions (move up, down, left, right, or stay). Each state transition yields a measurable reward (e.g., +1 for eating a pellet, +10 for eating a ghost, 0 otherwise), which the agent uses to learn an optimal policy that maximizes cumulative score. We will implement both a tabular Q-learning approach (suitable for smaller mazes) and a deep reinforcement learning method (using a neural network to approximate Q-values) to handle larger or more complex scenarios. Performance will be assessed by tracking total reward, survival time, and win rate across multiple episodes. Additionally, we plan to test the agents on new or modified mazes, measuring their ability to generalize under varying conditions. By systematically evaluating these algorithms, we aim to identify the most effective solution for real-time decision-making in Pac-Man and demonstrate how different RL strategies perform in a dynamic, adversarial environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "### Reinforcement Learning in Pacman\n",
    "Reinforcement learning (RL) is a branch of machine learning in which an agent learns to performs actions in an environment in order to maximize a cumulative reward <a name=\"suttonnote\"></a>[<sup>[4]</sup>](#sutton&barto). The agent iteratively explores the environment, observing the outcomes, rewards and penalties, of its actions. Over time, it refines a policy, which specifies the actions to take in each state to optimize long term return. Early RL methods, such as Q-learning and SARSA, rely on tabular representations of the value function Q(s,a) to estimate how desirable each action a is in a given state s. These approaches are effective in smaller domans, but become less reliable as the state space grows. This limitation has led to the integration of deep learning, more specifically Deep Reinforcement Learning (DRL)<a name=\"Arulkumarannote\"></a>[<sup>[1]</sup>](#arulkamaran). DRL leverages neural networks to approximate the value function or policy, allowing the agent to handle high-dimesional states <a name=\"mnihnote\"></a>[<sup>[1]</sup>](#mnih).\n",
    "\n",
    "Pac-Man is a classic arcade game featuring a grid-based maze. The agent (Pac-Man) navigates the maze to collect pellets while avoiding or chasing four ghosts. This environment offers RL challenges:\n",
    "- Discrete Action Space: Pac-Man can move in five different directions (up, down, left, right, stay)\n",
    "- Reward Structure: The game provides immediate rewards (pellets), high-value rewards (power pellets), and strong penalties (losing a life).\n",
    "- Adversial Dynamics: Ghosts actively pursue Pac-Man\n",
    "Because of these properties, Pac-Man is an ideal environment setup for RL. \n",
    "\n",
    "### Prior Work\n",
    "\n",
    "Pac-Man has been a popular benchmark for artificial intelligence (AI) research since the 1980s, initially explored through rule-based or search-based methods (e.g., depth-first search and A*) and later through classical reinforcement learning (RL). Early RL applications often employed tabular Q-learning on simplified mazes, where state representations included Pac-Man’s coordinates, ghost positions, and pellet locations. However, this approach suffered from a combinatorial explosion in larger or more complex layouts. To reduce the dimensionality, some researchers employed hand-engineered features—like “ghost proximity” or “pellet distance”—rather than storing value estimates for every possible state. Around the same time, Ms. Pac-Man emerged as a competition environment <a name=\"lucasnote\"></a>[<sup>[2]</sup>](#lucas), prompting novel strategies such as neuroevolution and hybrid RL techniques, which further demonstrated the need for more scalable, generalizable methods\n",
    "\n",
    "As deep learning gained traction, Deep Q-Networks (DQN) were introduced, showing human-level control in various Atari games <a name=\"mnihnote\"></a>[<sup>[3]</sup>](#mnih). This success naturally extended to Pac-Man, where deep RL agents learned to approximate Q-values using convolutional neural networks, bypassing manual feature engineering. The advent of DQN variants like Double DQN, Prioritized Experience Replay, and policy-gradient methods further improved stability and performance. \n",
    "\n",
    "### Challenges\n",
    "\n",
    "Pac-Man is simpler than real-world environments, but still offers many challenges:\n",
    "- Exponential State Space: The number of possible states grows quickly with the maze size, the postions of Pac-Man, number of ghosts, and remaining pellets.\n",
    "- Sparse Rewards: Collecting pellets is straightforward, but learning optimal strategies like chasing ghosts with higher power-pellet rewards can be challenging.\n",
    "- Exploration vs Exploitation: Pac-Man may be drawn to safer paths, but effective policies sometimes require risk taking like high-value power pellets.\n",
    "- Computational Cost: DRL can be computationally expensive, which will require careful hyperparameter tuning and potentially large-scale training to converge to optimal solutions <a name=\"Arulkumarannote\"></a>[<sup>[1]</sup>](#arulkamaran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "This project aims to build an intelligent agent that learns an optimal policy to play Pac-Man, maximizing score by collecting pellets while avoiding or chasing ghosts. The goal is to maximize the expected cumulative reward, where each pellet contributes to the score and collisions with ghosts incur penalties or game termination. The problem is quantifiable because it can be modeled as a Markov Decision Process (MDP): each state (positions of Pac-Man, ghosts, and remaining pellets) transitions to a new state with a probabilistic reward. It is measurable through performance metrics like average score, survival time, and win rate, all of which can be tracked across multiple episodes. Finally, it is replicable since Pac-Man’s environment, action space, and reward functions can be consistently re-initialized or varied in a controlled manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Data is empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In order to solve this problem as a MDP, we can compare Q-learning and deep reinforcement learning approaches to help solve our problem. Our set of states reflect different possible snapshots of the game; for example, each different position of Pacman and all the ghosts can be a different state, each dot and whether it has been eaten or not can also be information about a state, and so on. Our action space is discrete and can be broken down into directional movements of stay, up, down, left, and right.  \n",
    "\n",
    "##### Q-learning (benchmark)  \n",
    "Given a particular action, the value estimate of our state is represented as Q(s, a). We can build a Q-table that stores the Q-values of Q(s, a) for all pairs of states and actions. Q-learning enables us to update the Q-table with more accurate values as the agent explores/exploits the environment based on actions chosen from an epsilon greedy strategy. Thus, by choosing the maximum Q-value, we also are able to maximize our rewards. This approach will likely see better performance in smaller Pacman maps where the Q-table does not grow too large.  \n",
    "\n",
    "##### Deep reinforcement learning (solution)  \n",
    "As opposed to using a Q-table, deep reinforcement learning can estimate Q(s, a) instead by using a neural network with a state as its input and Q(s, a) for all actions. Then again, by choosing the maximum estimated Q-value, we should be able to maximize the rewards. This solution may be better suited for larger Pacman games where there are many more states that are difficult to fit in a Q-table.  \n",
    "\n",
    "##### Value iteration  \n",
    "If we define all the states, actions, and transition functions, we may be able to use value iteration to estimate the value of every state. To design all the transition functions, we could consider an example such as Pacman moving to the right once, which brings us to a new state where Pacman is now one right step over. If there was a dot in that location, we can say the reward for that transition is +1, and otherwise +0. By coming up with all possible transition functions and gathering all the information about the environment, we can obtain an optimal policy that chooses the best action based on the determined values of the states. However, this would likely be difficult for larger maps of Pacman where there are many different moving parts.  \n",
    " \n",
    "To test the performance of Q-learning and deep reinforcement learning, we can test the agent in an unknown map where we switch up the walls or pathways. We can also vary our value of epsilon to determine what ratio of exploration and exploitation is optimal. We can use a basic Q-learning algorithm as our benchmark algorithm.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In order to quantify the performance of our algorithms, we can attribute a point system that aligns with the regular rules of Pacman. For example, if Pacman moves into a new space and eats a dot, then we can give a reward of +1. However, if Pacman moves into a new space and doesn’t eat a dot, or Pacman stays in the same position, we can give a reward of +0. If Pacman is able to eat a ghost (after consuming a “power pellet”), we can give a higher reward, such as +10. This may incentivize our agent to ensure that all the ghosts are eaten before all the dots are eaten to maximize the reward. We could potentially limit the number of actions Pacman can take to see a difference in performance between algorithms, which can be equivalent to acting under a policy for a specified number of steps. Therefore, the algorithm that obtains a higher cumulative reward can be considered as the more optimal algorithm for our problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Probably you should include a learning curve to demonstrate how much better the model gets as you increase the number of trials\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Generally reinforement learning tasks may require a huge amount of training, so extensive grid search is unlikely to be possible. However expoloring a few reasonable hyper-parameters may still be possible.  Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?  Or you compare a completely different approach/alogirhtm to the problem? Whatever, this stuff is just serving suggestions.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "\n",
    "### Future work\n",
    "Looking at the limitations and/or the toughest parts of the problem and/or the situations where the algorithm(s) did the worst... is there something you'd like to try to make these better.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Our project mainly focuses on developing and comparing AI's with different machine learning algorithms, but there are some ethical consequences we must consider. One concern is that the AI agent may develop exploitative strategies if they were to find a loophold in the reward system, they might try to stall and play as safe as possible instead of finding the best way to get a high score, or manipulate game mechanics and bug out the game. We will adjust the reward system and take the game's bugs into consideration when creating our machine learning models to mitigate these issues.\n",
    "\n",
    "We will also take fairness into consideration when comparing different machine learning methods. Each different AI will be tested under identical conditions (maps, difficulty, etc.), ensuring that no AI has an advantage over another in our performance metrics. \n",
    "\n",
    "While Pac-Man is a clsoed enviornment, the AI concepts we develop could apply to real-world situations, we will take this into consideration when we create our modelss so that we can avoid this as much as possible.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"Arulkumarannote\"></a>1: Arulkumaran, K., Deisenroth, M. P., Brundage, M., & Bharath, A. A. (2017). A Brief Survey of Deep Reinforcement Learning. IEEE Signal Processing Magazine, 34(6), 26–38.<br> \n",
    "<a name=\"lucasnote\"></a>2: Lucas, S. M. (2007). Ms. Pac-Man competition. IEEE Symposium on Computational Intelligence and Games, 158–159. <br>\n",
    "<a name=\"mnihnote\"></a>3: Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533.<br>\n",
    "<a name=\"suttonnote\"></a>4: Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
